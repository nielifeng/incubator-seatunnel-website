<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.16">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Apache SeaTunnel RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Apache SeaTunnel Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/user_cases/rss.xml" title="Apache SeaTunnel RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/user_cases/atom.xml" title="Apache SeaTunnel Atom Feed">
<script src="https://hm.baidu.com/hm.js?33a9aab233e1082f91e4e347ad716701" async></script><title data-rh="true">Blog | Apache SeaTunnel</title><meta data-rh="true" property="og:title" content="Blog | Apache SeaTunnel"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" property="og:url" content="https://seatunnel.apache.org/blog/page/2"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/image/favicon.ico"><link data-rh="true" rel="canonical" href="https://seatunnel.apache.org/blog/page/2"><link data-rh="true" rel="alternate" href="https://seatunnel.apache.org/blog/page/2" hreflang="en"><link data-rh="true" rel="alternate" href="https://seatunnel.apache.org/zh-CN/blog/page/2" hreflang="zh-CN"><link data-rh="true" rel="alternate" href="https://seatunnel.apache.org/blog/page/2" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.41b6544e.css">
<link rel="preload" href="/assets/js/runtime~main.cca6282f.js" as="script">
<link rel="preload" href="/assets/js/main.80d5c78f.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><div class="announcementBar_IbjG" style="background-color:rgb(70, 125, 175, 0.8)" role="banner"><div class="announcementBarContent_KsVm">ü§î Have queries regarding Apache SeaTunnel, Join Slack channel to discuss them join <a target="_blank" rel="noopener noreferrer" href="https://join.slack.com/t/apacheseatunnel/shared_invite/zt-123jmewxe-RjB_DW3M3gV~xL91pZ0oVQ">#SeaTunnel</a> channel! üåü</div></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/image/logo.png" alt="Apache SeaTunnel Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/image/logo.png" alt="Apache SeaTunnel Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Apache SeaTunnel</b></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/">Home</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" class="navbar__link">Document</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/2.3.0/about">2.3.0</a></li><li><a class="dropdown__link" href="/docs/2.3.0-beta/about">2.3.0-beta</a></li><li><a class="dropdown__link" href="/docs/2.2.0-beta/intro/about">2.2.0-beta</a></li><li><a class="dropdown__link" href="/docs/2.1.3/intro/about">2.1.3</a></li><li><a class="dropdown__link" href="/docs/2.1.2/intro/about">2.1.2</a></li><li><a class="dropdown__link" href="/docs/2.1.1/intro/about">2.1.1</a></li><li><a class="dropdown__link" href="/docs/2.1.0/introduction">2.1.0</a></li><li><a class="dropdown__link" href="/docs/1.x/introduction">1.x(Not Apache Release)</a></li><li><a class="dropdown__link" href="/docs/about">Next</a></li><li><a class="dropdown__link" href="/versions/">All versions</a></li></ul></div><a class="navbar__item navbar__link" href="/download">Download</a><a class="navbar__item navbar__link" href="/community/contribution_guide/contribute">Community</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/user_cases">UserCases</a><a class="navbar__item navbar__link" href="/team">Team</a><a class="navbar__item navbar__link" href="/user">Users</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" class="navbar__link">ASF</a><ul class="dropdown__menu"><li><a href="https://www.apache.org/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Foundation</a></li><li><a href="https://www.apache.org/licenses/" target="_blank" rel="noopener noreferrer" class="dropdown__link">License</a></li><li><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="dropdown__link">Events</a></li><li><a href="https://www.apache.org/security/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Security</a></li><li><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Sponsorship</a></li><li><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Thanks</a></li></ul></div><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/Apache IoTDB (Internet of Things Database) is a software system that integrates the collection">SeaTunnel supports IoTDB to implement IoT data synchronization</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/Apache SeaTunnel Committer | Zongwen Li">SeaTunnel engine, designed for tens-of-billions data integration</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/During the joint Apache SeaTunnel &amp; IoTDB Meetup on October 15,">Mafengwo finally chose Apache SeaTunnel after analyzing these 9 points of how it works!</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/2022/09/20/A-tutorial-to-help-you develop-a-SeaTunnel-Connector-hand-by-hand-while-avoiding -pitfalls">A tutorial to help you develop a SeaTunnel Connector hand-by-hand while avoiding pitfalls</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/2022/09/19/Code-Demo-for-SeaTunnel-Connector-Development-Process">Code Demo for SeaTunnel Connector Development Process</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/SeaTunnel Application and Refactoring at Kidswant">SeaTunnel Application and Refactoring at Kidswant</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-05-01T00:00:00.000Z" itemprop="datePublished">May 1, 2022</time> ¬∑ <!-- -->10 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/0-38f7968af0b7239e9d427a85adee4452.png" width="1920" height="1080"></p><p>At the Apache SeaTunnel (Incubating) Meetup in April, Yuan Hongjun, a big data expert and OLAP platform architect at Kidswant, shared a topic of SeaTunnel Application and Refactoring at Kidswant.</p><p>The presentation contains five parts.</p><ul><li>Background of the introduction of Apache SeaTunnel (Incubating) by Kidswant</li><li>A comparison of mainstream tools for big data processing</li><li>The implementation of Apache SeaTunnel (Incubating)</li><li>Common problems in Apache SeaTunnel (Incubating) refactoring</li><li>Predictions on the future development of Kidswant</li></ul><p><img loading="lazy" src="/assets/images/0-1-4c853aa726b29acc5954ba53240dc2b8.png" width="2578" height="2567"></p><p>Yuan Hongjun, Big data expert, OLAP platform architect of Kidswant. He has many years of experience in big data platform development and management, and has rich research experience in data assets, data lineage mapping, data governance, OLAP, and other fields.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="01-background">01 Background<a class="hash-link" href="#01-background" title="Direct link to heading">‚Äã</a></h2><p><img loading="lazy" src="/assets/images/1-198d9a9b685f80a9814d5620d1194355.png" width="1166" height="720"></p><p>At present, Kidswant‚Äôs OLAP platform consists of seven parts: metadata layer, task layer, storage layer, SQL layer, scheduling layer, service layer, and monitoring layer. This sharing focuses on offline tasks in the task layer.</p><p>In fact, Kidswant had a complete internal collection and push system, but due to some historical legacy issues, the company‚Äôs existing platform could not quickly support the OLAP platform getting online, so at that time the company had to abandon its own platform and start developing a new system instead.
There were three options in front of OLAP at the time.</p><p>1, Re-develop the collection and push system.</p><p>2„ÄÅSelf-R&amp;D.</p><p>3, Participate in open source projects.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="02-big-data-processing-mainstream-tools-comparison">02 Big data processing mainstream tools comparison<a class="hash-link" href="#02-big-data-processing-mainstream-tools-comparison" title="Direct link to heading">‚Äã</a></h2><p>These three options have their own pros and cons. Carrying re-research and development based on the collection and push system is convenient for us to take advantage of the experience of previous results and avoid repeatedly stepping into the pit. But the disadvantage is that it requires a large amount of code, time, a longer research period, and with less abstract code and lots of customized functions bound to the business, it‚Äôs difficult to do the re-development.</p><p>If completely self-developed, though the development process is autonomous and controllable, some engines such as Spark can be done to fit our own architecture, while the disadvantage is that we may encounter some unknown problems.</p><p>For the last choice, if we use open-source frameworks, the advantage is that there is more abstract code, and the framework can be guaranteed in terms of performance and stability after verification by other major companies. Therefore Kidswant mainly studied three open-source data synchronization tools, DATAX, Sqoop, and SeaTunnel in the early stages of OLAP data synchronization refactoring.</p><p><img loading="lazy" src="/assets/images/2-fa4598ecce2b564564a8dfb2f7ccae87.png" width="919" height="720"></p><p>From the diagram we can see that Sqoop‚Äôs main function is data synchronization for RDB, and its implementation is based on MAP/REDUCE. Sqoop has rich parameters and command lines to perform various operations. The advantage of Sqoop is that it fits Hadoop ecology, and already supports most of the conversion from RDB to HIVE arbitrary source, with a complete set of commands and APIs.</p><p>The disadvantages are that Sqoop only supports RDB data synchronization and has some limitations on data files, and there is no concept of data cleansing yet.</p><p><img loading="lazy" src="/assets/images/3-53716e33ac191ff35ff66591ba4e1711.png" width="1174" height="720"></p><p>DataX mainly aims at synchronizing data from any source by configurable files + multi-threading, which runs three main processes: Reader, Framework, and Writer, where Framework mainly plays the role of communication and leaving empty space.</p><p>The advantage of DataX is that it uses plug-in development, has its own flow control and data control, and is active in the community, with DataX‚Äôs official website offering data pushes from many different sources. The disadvantage of DataX, however, is that it is memory-based and there may be limitations on the amount of data available.</p><p><img loading="lazy" src="/assets/images/4-1897837b0d7e4b52bc086c94b5aa4aea.png" width="1025" height="720"></p><p>Apache SeaTunnel (Incubating) also does data synchronization from any source and implements the process in three steps: source, transform and sink based on configuration files, Spark or Flink. </p><p>The advantage is that the current 2.1.0 version has a very large number of plug-ins and source pushes, based on the idea of plug-ins also makes it very easy to extend and embrace Spark and Flink while with a distributed architecture. The only downside to Apache SeaTunnel (Incubating) is probably the lack of IP calls at the moment and the need to manage the UI interface by yourself.</p><p>In summary, although Sqoop is distributed, it only supports data synchronization between RDB and HIVE, Hbase and has poor scalability, which is not convenient for re-development. DataX is scalable and stable overall, but because it is a standalone version, it cannot be deployed in a distributed cluster, and there is a strong dependency between data extraction capability and machine performance. SeaTunnel, on the other hand, is similar to DataX and makes up for the flaw of non-distributed DataX. It also supports real-time streaming, and the community is highly active as a new product. We chose SeaTunnel based on a number of factors such as whether it supported distributed or not, and whether it needed to be deployed on a separate machine.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="03-implementation">03 Implementation<a class="hash-link" href="#03-implementation" title="Direct link to heading">‚Äã</a></h2><p>On the Apache SeaTunnel (Incubating) website, we can see that the basic process of Apache SeaTunnel (Incubating) consists of three parts: source, transform and sink. According to the guidelines on the website, Apache SeaTunnel (Incubating) requires a configuration script to start, but after some research, we found that the final execution of Apache SeaTunnel (Incubating) is bansed on an application submitted by spark-submit that relies on the config file.</p><p>This initialization, although simple, has the problem of having to rely on the config file, which is generated and then cleared after each run, and although it can be dynamically generated in the scheduling script, it raises two questions: 1) whether frequent disk operations make sense; and 2) whether there is a more efficient way to support Apache SeaTunnel (Incubating).</p><p><img loading="lazy" src="/assets/images/5-c2ec6a581e98f15b50a0925325be6acf.png" width="1196" height="720"></p><p>With these considerations in mind, we added a Unified Configuration Template Platform module to the final design solution. Scheduling is done by initiating a commit command, and Apache SeaTunnel (Incubating) itself pulls the configuration information from the unified configuration template platform, then loads and initializes the parameters.</p><p><img loading="lazy" src="/assets/images/6-5d4653fed2f38a65b3e8c04428e1d53e.png" width="1048" height="720"></p><p>The diagram above shows the business process for Kidswant‚Äôs OLAP, which is divided into three sections. The overall flow of data from Parquet, i.e. Hive, through the Parquet tables to KYLIN and CK source.</p><p><img loading="lazy" src="/assets/images/7-2eed2f9ff97c6bfcc712e992568dd102.png" width="1080" height="687"></p><p>This is the page where we construct the model, which is generated mainly through drag and drop, with some transactional operations between each table, and micro-processing for Apache SeaTunnel (Incubating) on the right.</p><p><img loading="lazy" src="/assets/images/8-8d444012b45ea1a51088db223d29efa6.png" width="1247" height="596"></p><p>So we end up submitting the commands as above, where the first one marked in red is <!-- -->[-conf customconfig/jars]<!-- -->, referring to the fact that the user can then unify the configuration template platform for processing, or specify it separately when modeling. The last one marked in red is <!-- -->[421 $start_time $end_time $taskType]<!-- --> Unicode, which is a unique encoding.</p><p>Below, on the left, are the 38 commands submitted by our final dispatch script. Below, on the right, is a modification made for Apache SeaTunnel (Incubating), and you can see a more specific tool class called WaterdropContext. It can first determine if Unicode exists and then use Unicode_code to get the configuration information for the different templates, avoiding the need to manipulate the config file.</p><p>In the end, the reportMeta is used to report some information after the task is completed, which is also done in Apache SeaTunnel (Incubating).</p><p><img loading="lazy" src="/assets/images/9-54db46eb9209112af1884f1ef6454536.png" width="720" height="752"></p><p><img loading="lazy" src="/assets/images/10-0472de8100ba32800e40ea9e86f3b35f.png" width="1280" height="585"></p><p><img loading="lazy" src="/assets/images/11-910cd332e1ebf6b9d7d3ea9186a5c8fa.png" width="1280" height="512"></p><p>In the finalized config file as above, it is worth noting that in terms of transforms, Kidswant has made some changes. The first is to do desensitization for mobile phones or ID numbers etc. If the user specifies a field, they do it by field, if not they will scan all fields and then desensitize and encrypt them according to pattern matching.</p><p>Second, transform also supports custom processing, as mentioned above when talking about OLAP modeling. With the addition of HideStr, the first ten fields of a string of characters can be retained and all characters at the back encrypted, providing some security in the data.</p><p>Then, on the sink side, we added pre_sql in order to support the idempotency of the task, which is mainly done for tasks such as data deletion, or partition deletion, as the task cannot be run only once during production, and this design needed to account for the data deviation and correctness once operations such as reruns or complement occur.</p><p>On the right side of the diagram, on the Sink side of a Clickhouse, we have added an is_senseless_mode, which forms a read/write senseless mode, where the user does not perceive the whole area when querying and complementing but uses the CK partition conversion, i.e. the command called MOVE PARTITION TO TABLE to operate.</p><p>A special note here is the Sink side of KYLIN. KYLIN is a very special source with its own set of data entry logic and its monitoring page, so the transformation we have done on KYLIN is simply a call to its API operation and a simple API call and constant polling of the state when using KYLIN, so the resources for KYLIN are limited in the Unified Template Configuration platform.</p><p><img loading="lazy" src="/assets/images/12-859f200352aab300d0e421ba01e86da6.png" width="1249" height="586"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="04-common-problems-about-the-apache-seatunnel-incubating-transformation">04 Common problems about the Apache SeaTunnel (Incubating) transformation<a class="hash-link" href="#04-common-problems-about-the-apache-seatunnel-incubating-transformation" title="Direct link to heading">‚Äã</a></h2><h4 class="anchor anchorWithStickyNavbar_mojV" id="01-oom--too-many-parts">01 OOM &amp; Too Many Parts<a class="hash-link" href="#01-oom--too-many-parts" title="Direct link to heading">‚Äã</a></h4><p>The problem usually arises during the Hive to Hive process, even if we go through automatic resource allocation, but there are cases where the data amount suddenly gets bigger, for example after holding several events. Such problems can only be avoided by manually and dynamically tuning the reference and adjusting the data synchronization batch time. In the future, we may try to control the data volume to achieve fine control.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="02-field-and-type-inconsistency-issues">02 Field and type inconsistency issues<a class="hash-link" href="#02-field-and-type-inconsistency-issues" title="Direct link to heading">‚Äã</a></h4><p>When the model runs, the user will make some changes to the upstream tables or fields that the task depends on, and these changes may lead to task failure if they are not perceived. The current solution is to rely on data lineage+ snapshots for advance awareness to avoid errors.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="03-custom-data-sources--custom-separators">03 Custom data sources &amp; custom separators<a class="hash-link" href="#03-custom-data-sources--custom-separators" title="Direct link to heading">‚Äã</a></h4><p>If the finance department requires a customized separator or jar information, the user can now specify the loading of additional jar information as well as the separator information themselves in the unified configuration template platform.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="04-data-skewing-issues">04 Data skewing issues<a class="hash-link" href="#04-data-skewing-issues" title="Direct link to heading">‚Äã</a></h4><p>This may be due to users setting their parallelism but not being able to do so perfectly. We haven‚Äôt finished dealing with this issue yet, but we may add post-processing to the Source module to break up the data and complete the skew.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="05-kylin-global-dictionary-lock-problem">05 KYLIN global dictionary lock problem<a class="hash-link" href="#05-kylin-global-dictionary-lock-problem" title="Direct link to heading">‚Äã</a></h4><p>As the business grows, one cube will not be able to meet the needs of the users, so it will be necessary to create more than one cube. If the same fields are used between multiple cubes, the problem of KYLIN global dictionary lock will be encountered. The current solution is to separate the scheduling time between two or more tasks, or if this is not possible, we can make a distributed lock control, where the sink side of KYLIN has to get the lock to run.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="05-an-outlook-on-the-future-of-kidswant">05 An outlook on the future of Kidswant<a class="hash-link" href="#05-an-outlook-on-the-future-of-kidswant" title="Direct link to heading">‚Äã</a></h2><ol><li>Multi-source data synchronization, maybe processing for RDB sources</li><li>Real-time Flink-based implementation</li><li>Take over the existing collection and scheduling platform (mainly to solve the problem of splitting library and tables)</li><li>Data quality verification, like some null values, the vacancy rate of the whole data, main time judgment, etc.</li></ol><p>This is all I have to share, I hope we can communicate more with the community in the future and make progress together, thanks!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/Innovation of Data Integration Technology in the Intelligent Era">Innovation of Data Integration Technology in the Intelligent Era</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-04-08T00:00:00.000Z" itemprop="datePublished">April 8, 2022</time> ¬∑ <!-- -->4 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="1" src="/assets/images/1-2af82fe19d2d0ad453f547d8c27647c4.png" width="1600" height="900"></p><p>As we know, only manageable, callable, computable, and magnetizable data resources can be deposited as assets. The interconnection of information systems has created a huge demand for multi-source and multidimensional data integration, which imposes strict requirements on data processing and integration tools.</p><p>In the era of intelligence, under the trends of ‚Äúsmart city‚Äù, ‚Äúsmart governance‚Äù, and ‚Äúintelligent products‚Äù, enterprises are mostly faced with the challenge of how to achieve efficient data push, improve platform quality, and ensure data security. Only by choosing the right data integration tools and platforms can data play a key role.</p><p>As a next-generation high-performance, distributed, and massive data integration framework, Apache SeaTunnel is committed to making data synchronization simpler and more efficient and accelerating the implementation of distributed data processing capabilities in the production environment.</p><p>At the Apache SeaTunnel Meetup (April 16, 2022), the community will invite experienced Apache SeaTunnel users to share the best practices of the project in intelligent production environments. In addition, there will be contributors to analyze the source code of Apache SeaTunnel, guiding you to have a comprehensive and in-depth understanding of this powerful data integration tool.</p><p>Whether you are a beginner who is interested in Apache SeaTunnel or users who encounter complex and difficult deployment problems in daily practice, you can come here to communicate with our instructors and get the answers you want.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="01-sign-up">01 Sign up<a class="hash-link" href="#01-sign-up" title="Direct link to heading">‚Äã</a></h2><p>Apache SeaTunnel Meetup | April online live registration has been started, hurry up and register!</p><p>Time: 2022‚Äì4‚Äì16 14:00‚Äì17:00</p><p>Format: live online</p><p>Click the link to register (free):¬†<a href="https://www.slidestalk.com/m/780" target="_blank" rel="noopener noreferrer">https://www.slidestalk.com/m/780</a></p><p>Join Slack:</p><p><a href="https://join.slack.com/t/apacheseatunnel/shared_invite/zt-10u1eujlc-g4E~ppbinD0oKpGeoo_dAw" target="_blank" rel="noopener noreferrer">https://join.slack.com/t/apacheseatunnel/shared_invite/zt-10u1eujlc-g4E~ppbinD0oKpGeoo_dAw</a></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="02-highlights">02 Highlights<a class="hash-link" href="#02-highlights" title="Direct link to heading">‚Äã</a></h2><ul><li>Detailed case study</li><li>Feature Analysis</li><li>Tips to avoid stepping into the pit from enterprises</li><li>Open-source community growth strategy</li><li>Face-to-face Q&amp;A with industry technical experts</li><li>Surprise gifts</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="03-event-agenda">03 Event Agenda<a class="hash-link" href="#03-event-agenda" title="Direct link to heading">‚Äã</a></h2><p>On the day of the event, big data engineers from Kidswant and oppo will share the front-line practical experience, and senior engineers from WhaleOps will give a ‚Äúhard-core‚Äù explanation of the important function updates of Apache SeaTunnel.</p><p><img loading="lazy" alt="2" src="/assets/images/2-a5ffa068e39b9488fa6ebdc44358b9a3.png" width="1080" height="1075"></p><p>Yuan Hongjun, Kidswant Big Data Expert, OLAP Platform Architect</p><p>Years of experience in R&amp;D and management of big data platforms, rich research experience in data assets, data linkage, data governance, OLAP, and other fields</p><p>Time: 14:00‚Äì14:40</p><p>Topic: Application Practice of Apache SeaTunnel in Kidswant</p><p>Speech outline: How to push data efficiently? How to improve the quality of the platform? How to ensure data security? What changes did Kidswant make to Apache SeaTunnel?</p><p><img loading="lazy" alt="3" src="/assets/images/3-22d7703611173db9493fd49adc5687ae.png" width="1080" height="1080"></p><p>Fan Jia, WhaleOps Senior Engineer</p><p>Time: 14:40‚Äì15:20</p><p>Topic: Clickhouse Bulk Load Implementation Based on Apache SeaTunnel</p><p>Speech outline: How to implement the bulk load data synchronization function of Clickhouse by extending the Connector of Apache SeaTunnel?</p><p><img loading="lazy" alt="4" src="/assets/images/4-acd48a1d76d384e7e930ab44862aded8.png" width="1080" height="1078"></p><p>Wang Zichao, Oppo Senior Backend Engineer</p><p>Time: 15:50‚Äì16:30</p><p>Topic: The technological innovation of oppo intelligent recommendation sample center based on Apache SeaTunnel</p><p>Speech outline: Introduce the evolution of oppo‚Äôs intelligent recommendation machine learning sample dealing process and the role of Apache SeaTunnel in it.</p><p>In addition to the wonderful speeches, a number of lucky draw sessions were also set up on the meetup. Anyone participating in the lucky draw will have the opportunity to win exquisite customized gifts from Apache SeaTunnel, so stay tuned~</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="about-seatunnel"><strong>About SeaTunnel</strong><a class="hash-link" href="#about-seatunnel" title="Direct link to heading">‚Äã</a></h2><p>SeaTunnel (formerly Waterdrop) is an easy-to-use, ultra-high-performance distributed data integration platform that supports real-time synchronization of massive amounts of data and can synchronize hundreds of billions of data per day in a stable and efficient manner.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="why-do-we-need-seatunnel"><strong>Why do we need SeaTunnel?</strong><a class="hash-link" href="#why-do-we-need-seatunnel" title="Direct link to heading">‚Äã</a></h2><p>SeaTunnel does everything it can to solve the problems you may encounter in synchronizing massive amounts of data.</p><ul><li>Data loss and duplication</li><li>Task buildup and latency</li><li>Low throughput</li><li>Long application-to-production cycle time</li><li>Lack of application status monitoring</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel-usage-scenarios"><strong>SeaTunnel Usage Scenarios</strong><a class="hash-link" href="#seatunnel-usage-scenarios" title="Direct link to heading">‚Äã</a></h2><ul><li>Massive data synchronization</li><li>Massive data integration</li><li>ETL of large volumes of data</li><li>Massive data aggregation</li><li>Multi-source data processing</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="features-of-seatunnel"><strong>Features of SeaTunnel</strong><a class="hash-link" href="#features-of-seatunnel" title="Direct link to heading">‚Äã</a></h2><ul><li>Rich components</li><li>High scalability</li><li>Easy to use</li><li>Mature and stable</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="how-to-get-started-with-seatunnel-quickly"><strong>How to get started with SeaTunnel quickly?</strong><a class="hash-link" href="#how-to-get-started-with-seatunnel-quickly" title="Direct link to heading">‚Äã</a></h2><p>Want to experience SeaTunnel quickly? SeaTunnel 2.1.0 takes 10 seconds to get you up and running.</p><p><a href="https://seatunnel.apache.org/docs/2.1.0/developement/setup" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/docs/2.1.0/developement/setup</a></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="how-can-i-contribute"><strong>How can I contribute?</strong><a class="hash-link" href="#how-can-i-contribute" title="Direct link to heading">‚Äã</a></h2><p>We invite all partners who are interested in making local open-source global to join the SeaTunnel contributors family and foster open-source together!</p><p>Submit an issue:</p><p><a href="https://github.com/apache/incubator-seatunnel/issues" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues</a></p><p>Contribute code to:</p><p><a href="https://github.com/apache/incubator-seatunnel/pulls" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pulls</a></p><p>Subscribe to the community development mailing list :</p><p><a href="mailto:dev-subscribe@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev-subscribe@seatunnel.apache.org</a></p><p>Development Mailing List :</p><p><a href="mailto:dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev@seatunnel.apache.org</a></p><p>Join Slack:</p><p><a href="https://join.slack.com/t/apacheseatunnel/shared_invite/zt-123jmewxe-RjB_DW3M3gV~xL91pZ0oVQ" target="_blank" rel="noopener noreferrer">https://join.slack.com/t/apacheseatunnel/shared_invite/zt-123jmewxe-RjB_DW3M3gV~xL91pZ0oVQ</a></p><p>Follow Twitter:</p><p><a href="https://twitter.com/ASFSeaTunnel" target="_blank" rel="noopener noreferrer">https://twitter.com/ASFSeaTunnel</a></p><p>Come and join us!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/2.1.0-Released-Apache-SeaTunnel-Incubating-First-Apache-Release-Refactors-Kernel-and-Supports-Flink-Overall">2.1.0 Released! Apache SeaTunnel(Incubating) First Apache Release Refactors Kernel and Supports Flink Overall</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-03-18T00:00:00.000Z" itemprop="datePublished">March 18, 2022</time> ¬∑ <!-- -->6 min read</div></header><div class="markdown" itemprop="articleBody"><p>On December 9, 2021, Apache SeaTunnel(Incubating) entered the Apache Incubator, and after nearly four months of endeavor by the community contributors, we passed the first Apache version control in one go and released it on March 18, 2022. This means that version 2.1.0 is an official release that is safe for corporate and individual users to use, which has been voted on by the Apache SeaTunnel(Incubating) community and the Apache Incubator.</p><p><strong>Note:</strong> A¬†<strong>software license</strong>¬†is a legal instrument governing the use or redistribution of software. A typical software license grants the¬†licensee, typically an¬†end-user, permission to use one or more copies of the software in ways where such a use would otherwise potentially constitute copyright infringement of the software owner&#x27;s¬†exclusive rights¬†under copyright. Effectively, a software license is a contract between the software developer and the user that guarantees the user will not be sued within the scope of the license. </p><p>Before and after entering the incubator, we spent a lot of time sorting through the external dependencies of the entire project to ensure compliance. It is important to note that the choice of License for open source software does not necessarily mean that the project itself is compliant. While the stringent version control process of ASF ensures compliance and legal distribution of the software license maximumly.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="release-note">Release Note<a class="hash-link" href="#release-note" title="Direct link to heading">‚Äã</a></h2><p>We bring the following <strong>key features</strong>to this release:</p><ol><li>The kernel of the microkernel plug-in architecture is overall optimized, which is mainly in Java. And a lot of improvements are made to command line parameter parsing, plug-in loading, etc. At the same time, the users (or contributors) can choose the language to develop plug-in extensions, which greatly reduces the development threshold of plug-ins.</li><li>Overall support for Flink, while the users are free to choose the underlying engine. This version also brings a large number of Flink plug-ins and welcomes anyone to contribute more.</li><li>Provide local development fast startup environment support (example), allow contributors or users quickly and smoothly start without changing any code to facilitate rapid local development debugging. This is certainly exciting news for contributors or users who need to customize their plugins. In fact, we&#x27;ve had a large number of contributors use this approach to quickly test the plugin in our pre-release testing.</li><li>With Docker container installation provided, users can deploy and install Apache SeaTunnel(Incubating) via Docker extremely fast, and we will iterate around Docker &amp; K8s in the future, any interesting proposal on this is welcomed.</li></ol><h2 class="anchor anchorWithStickyNavbar_mojV" id="specific-release-notes">Specific release notesÔºö<a class="hash-link" href="#specific-release-notes" title="Direct link to heading">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="features">[Features]<a class="hash-link" href="#features" title="Direct link to heading">‚Äã</a></h3><ul><li>Use JCommander to do command line parameter parsing, making developers focus on the logic itself.</li><li>Flink is upgraded from 1.9 to 1.13.5, keeping compatibility with older versions and preparing for subsequent CDC.</li><li>Support for Doris, Hudi, Phoenix, Druid, and other Connector plugins, and you can find complete plugin support here <a href="/blog/page/[https://github.com/apache/incubator-seatunnel#plugins-supported-by-seatunnel%5D(https://github.com/apache/incubator-seatunnel#plugins-supported-by-seatunnel)">plugins-supported-by-seatunnel</a>.</li><li>Local development extremely fast starts environment support. It can be achieved by using the example module without modifying any code, which is convenient for local debugging.</li><li>Support for installing and trying out Apache SeaTunnel(Incubating) via Docker containers.</li><li>SQL component supports SET statements and configuration variables.</li><li>Config module refactoring to facilitate understanding for the contributors while ensuring code compliance (License) of the project.</li><li>Project structure realigned to fit the new Roadmap.</li><li>CI&amp;CD support, code quality automation control (more plans will be carried out to support CI&amp;CD development).</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="acknowledgments">Acknowledgments<a class="hash-link" href="#acknowledgments" title="Direct link to heading">‚Äã</a></h2><p>Thanks to the following contributors who participated in this version release (GitHub IDs, in no particular order).</p><p>Al-assad, BenJFan, CalvinKirs, JNSimba, JiangTChen, Rianico, TyrantLucifer, Yves-yuan, ZhangchengHu0923, agendazhang, an-shi-chi-fan, asdf2014, bigdataf, chaozwn, choucmei, dailidong, dongzl, felix-thinkingdata, fengyuceNv, garyelephant, kalencaya, kezhenxu94, legendtkl, leo65535, liujinhui1994, mans2singh, marklightning, mosence, nielifeng, ououtt, ruanwenjun, simon824, totalo, wntp, wolfboys, wuchunfu, xbkaishui, xtr1993, yx91490, zhangbutao, zhaomin1423, zhongjiajie, zhuangchong, zixi0825.</p><p>Also sincere gratitude to our Mentors: Zhenxu Ke, Willem Jiang, William Guo, LiDong Dai, Ted Liu, Kevin, JB for their help!</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="planning-for-the-next-few-releases">Planning for the next few releases:<a class="hash-link" href="#planning-for-the-next-few-releases" title="Direct link to heading">‚Äã</a></h2><ul><li>CDC support.</li><li>Support for the monitoring system.</li><li>UI system support.</li><li>More Connector and efficient Sink support, such as ClickHouse support will be available in the next release soon.
The follow-up <strong>Features</strong> are decided by the community consensus, and we sincerely appeal to more participation in the community construction.</li></ul><p>We need your attention and contributions:)</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="community-status">Community Status<a class="hash-link" href="#community-status" title="Direct link to heading">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="recent-development">Recent Development<a class="hash-link" href="#recent-development" title="Direct link to heading">‚Äã</a></h3><p>Since entering the Apache incubator, the contributor group has grown from 13 to 55 and continues to grow, with the average weekly community commits remaining at 20+. </p><p>Three contributors from different companies (Lei Xie, HuaJie Wang, Chunfu Wu) have been invited to become Committers on account of their contributions to the community. </p><p>We held two Meetups, where instructors from Bilibili, OPPO, Vipshop, and other companies shared their large-scale production practices based on SeaTunnel in their companies (we will hold one meetup monthly in the future, and welcome SeaTunnel users or contributors to come and share their stories about SeaTunnel).</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="users-of-apache-seatunnelincubating">Users of Apache SeaTunnel(Incubating)<a class="hash-link" href="#users-of-apache-seatunnelincubating" title="Direct link to heading">‚Äã</a></h3><p>Note: Only registered users are included.</p><p>Registered users of Apache SeaTunnel(Incubating) are shown below. If you are also using Apache SeaTunnel(Incubating), too, welcome to register on <a href="https://github.com/apache/incubator-seatunnel/issues/686" target="_blank" rel="noopener noreferrer">Who is using SeaTunne</a>!</p><div align="center"><img src="/image/20220321/1.png"></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="ppmcs-word">PPMC&#x27;s Word<a class="hash-link" href="#ppmcs-word" title="Direct link to heading">‚Äã</a></h2><p>LiFeng Nie, PPMC of Apache SeaTunnel(Incubating), commented on the first Apache version release. </p><p>From the first day entering Apache Incubating, we have been working hard to learn the Apache Way and various Apache policies. Although the first release took a lot of time (mainly for compliance), we think it was well worth it, and that&#x27;s one of the reasons we chose to enter Apache. We need to give our users peace of mind, and Apache is certainly the best choice, with its almost demanding license control that allows users to avoid compliance issues as much as possible and ensure that the software is circulating reasonably and legally. In addition, its practice of the Apache Way, such as public service mission, pragmatism, community over code, openness and consensus decision-making, and meritocracy, can drive the Apache SeaTunnel(Incubating) community to become more open, transparent, and diverse.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/2-1-0">2.1.0</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/release">Release</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/hdfs-to-clickhouse">How to quickly import data from HDFS into ClickHouse</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-12-30T00:00:00.000Z" itemprop="datePublished">December 30, 2021</time> ¬∑ <!-- -->6 min read</div></header><div class="markdown" itemprop="articleBody"><p>ClickHouse is a distributed columnar DBMS for OLAP. Our department has now stored all log data related to data analysis in ClickHouse, an excellent data warehouse, and the current daily data volume has reached 30 billion.</p><p>The experience of data processing and storage introduced earlier is based on real-time data streams. The data is stored in Kafka. We use Java or Golang to read, parse, and clean the data from Kafka and write it into ClickHouse, so that the data can be stored in ClickHouse. Quick access. However, in the usage scenarios of many students, the data is not real-time, and it may be necessary to import the data in HDFS or Hive into ClickHouse. Some students implement data import by writing Spark programs, so is there a simpler and more efficient way?</p><p>At present, there is a tool <strong>Seatunnel</strong> in the open source community, the project address <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a>, can quickly Data in HDFS is imported into ClickHouse.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="hdfs-to-clickhouse">HDFS To ClickHouse<a class="hash-link" href="#hdfs-to-clickhouse" title="Direct link to heading">‚Äã</a></h2><p>Assuming that our logs are stored in HDFS, we need to parse the logs and filter out the fields we care about, and write the corresponding fields into the ClickHouse table.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="log-sample">Log Sample<a class="hash-link" href="#log-sample" title="Direct link to heading">‚Äã</a></h3><p>The log format we store in HDFS is as follows, which is a very common Nginx log</p><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token number">10.41</span><span class="token plain">.1.28 github.com </span><span class="token number">114.250</span><span class="token plain">.140.241 </span><span class="token number">0</span><span class="token plain">.001s </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;127.0.0.1:80&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">26</span><span class="token plain">/Oct/2018:03:09:32 +0800</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;GET /Apache/Seatunnel HTTP/1.1&quot;</span><span class="token plain"> </span><span class="token number">200</span><span class="token plain"> </span><span class="token number">0</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;-&quot;</span><span class="token plain"> - </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Dalvik/2.1.0 (Linux; U; Android 7.1.1; OPPO R11 Build/NMF26X)&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;196&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;-&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;mainpage&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;443&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;-&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;172.16.181.129&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="clickhouse-schema">ClickHouse Schema<a class="hash-link" href="#clickhouse-schema" title="Direct link to heading">‚Äã</a></h3><p>Our ClickHouse table creation statement is as follows, our table is partitioned by day</p><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE cms.cms_msg</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> Date, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    datetime DateTime, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    url String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    request_time Float32, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    status String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">hostname</span><span class="token plain"> String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    domain String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remote_addr String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data_size Int32, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    pool String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ENGINE </span><span class="token operator">=</span><span class="token plain"> MergeTree PARTITION BY </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> ORDER BY </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> SETTINGS index_granularity </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">16384</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel-with-clickhouse">Seatunnel with ClickHouse<a class="hash-link" href="#seatunnel-with-clickhouse" title="Direct link to heading">‚Äã</a></h2><p>Next, I will introduce to you in detail how we can meet the above requirements through Seatunnel and write the data in HDFS into ClickHouse.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel">Seatunnel<a class="hash-link" href="#seatunnel" title="Direct link to heading">‚Äã</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> is a very easy-to-use, high-performance, real-time data processing product that can deal with massive data. It is built on Spark. Seatunnel has a very rich set of plugins that support reading data from Kafka, HDFS, Kudu, performing various data processing, and writing the results to ClickHouse, Elasticsearch or Kafka.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="prerequisites">Prerequisites<a class="hash-link" href="#prerequisites" title="Direct link to heading">‚Äã</a></h3><p>First we need to install Seatunnel, the installation is very simple, no need to configure system environment variables</p><ol><li>Prepare the Spark environment</li><li>Install Seatunnel</li><li>Configure Seatunnel</li></ol><p>The following are simple steps, the specific installation can refer to <a href="/docs/quick-start">Quick Start</a></p><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token builtin class-name" style="color:rgb(189, 147, 249)">cd</span><span class="token plain"> /usr/local</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">wget</span><span class="token plain"> https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">tar</span><span class="token plain"> -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">wget</span><span class="token plain"> https://github.com/InterestingLab/seatunnel/releases/download/v1.1.1/seatunnel-1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">unzip</span><span class="token plain"> seatunnel-1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">cd</span><span class="token plain"> seatunnel-1.1.1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">vim</span><span class="token plain"> config/seatunnel-env.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Specify the Spark installation path</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token assign-left variable" style="color:rgb(189, 147, 249);font-style:italic">SPARK_HOME</span><span class="token operator">=</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">${SPARK_HOME</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">:-</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">/</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">usr</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">/</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">local</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">/</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">spark-2.2.0-bin-hadoop2.7}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel-pipeline">seatunnel Pipeline<a class="hash-link" href="#seatunnel-pipeline" title="Direct link to heading">‚Äã</a></h3><p>We only need to write a configuration file of seatunnel Pipeline to complete the data import.</p><p>The configuration file consists of four parts, Spark, Input, filter and Output.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="spark">Spark<a class="hash-link" href="#spark" title="Direct link to heading">‚Äã</a></h4><p>This part is the related configuration of Spark, which mainly configures the size of the resources required for Spark to execute.</p><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;seatunnel&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;1g&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="input">Input<a class="hash-link" href="#input" title="Direct link to heading">‚Äã</a></h4><p>This part defines the data source. The following is a configuration example for reading data in text format from HDFS files.</p><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">input </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hdfs </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        path </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hdfs://nomanode:8020/rowlog/accesslog&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access_log&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">format</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;text&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="filter">Filter<a class="hash-link" href="#filter" title="Direct link to heading">‚Äã</a></h4><p>In the Filter section, here we configure a series of transformations, including regular parsing to split the log, time transformation to convert HTTPDATE to the date format supported by ClickHouse, type conversion to Number type fields, and field filtering through SQL, etc.</p><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Parse raw logs using regular expressions</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;raw_message&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;%{IP:ha_ip}\\s%{NOTSPACE:domain}\\s%{IP:remote_addr}\\s%{NUMBER:request_time}s\\s\&quot;%{DATA:upstream_ip}\&quot;\\s\\[%{HTTPDATE:timestamp}\\]\\s\&quot;%{NOTSPACE:method}\\s%{DATA:url}\\s%{NOTSPACE:http_ver}\&quot;\\s%{NUMBER:status}\\s%{NUMBER:body_bytes_send}\\s%{DATA:referer}\\s%{NOTSPACE:cookie_info}\\s\&quot;%{DATA:user_agent}\&quot;\\s%{DATA:uid}\\s%{DATA:session_id}\\s\&quot;%{DATA:pool}\&quot;\\s\&quot;%{DATA:tag2}\&quot;\\s%{DATA:tag3}\\s%{DATA:tag4}&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Convert data in &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; format to</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Data in &quot;yyyy/MM/dd HH:mm:ss&quot; format</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;timestamp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;datetime&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;yyyy/MM/dd HH:mm:ss&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Use SQL to filter the fields of interest and process the fields</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># You can even filter out data you don&#x27;t care about by filter conditions</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;select substring(date, 1, 10) as date, datetime, hostname, url, http_code, float(request_time), int(data_size), domain from access&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="output">Output<a class="hash-link" href="#output" title="Direct link to heading">‚Äã</a></h4><p>Finally, we write the processed structured data to ClickHouse</p><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">host</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;your.clickhouse.host:8123&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;seatunnel&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access_log&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;date&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;datetime&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hostname&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;uri&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;http_code&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;request_time&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;data_size&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;domain&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;username&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;password&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="running-seatunnel">Running seatunnel<a class="hash-link" href="#running-seatunnel" title="Direct link to heading">‚Äã</a></h3><p>We combine the above four-part configuration into our configuration file <code>config/batch.conf</code>.</p><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">vim</span><span class="token plain"> config/batch.conf</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;seatunnel&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;1g&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hdfs </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        path </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hdfs://nomanode:8020/rowlog/accesslog&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access_log&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">format</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;text&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Parse raw logs using regular expressions</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;raw_message&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;%{IP:ha_ip}\\s%{NOTSPACE:domain}\\s%{IP:remote_addr}\\s%{NUMBER:request_time}s\\s\&quot;%{DATA:upstream_ip}\&quot;\\s\\[%{HTTPDATE:timestamp}\\]\\s\&quot;%{NOTSPACE:method}\\s%{DATA:url}\\s%{NOTSPACE:http_ver}\&quot;\\s%{NUMBER:status}\\s%{NUMBER:body_bytes_send}\\s%{DATA:referer}\\s%{NOTSPACE:cookie_info}\\s\&quot;%{DATA:user_agent}\&quot;\\s%{DATA:uid}\\s%{DATA:session_id}\\s\&quot;%{DATA:pool}\&quot;\\s\&quot;%{DATA:tag2}\&quot;\\s%{DATA:tag3}\\s%{DATA:tag4}&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Convert data in &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; format to</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Data in &quot;yyyy/MM/dd HH:mm:ss&quot; format</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;timestamp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;datetime&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;yyyy/MM/dd HH:mm:ss&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Use SQL to filter the fields of interest and process the fields</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># You can even filter out data you don&#x27;t care about by filter conditions</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;select substring(date, 1, 10) as date, datetime, hostname, url, http_code, float(request_time), int(data_size), domain from access&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">host</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;your.clickhouse.host:8123&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;seatunnel&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access_log&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;date&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;datetime&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hostname&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;uri&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;http_code&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;request_time&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;data_size&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;domain&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;username&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;password&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Execute the command, specify the configuration file, and run Seatunnel to write data to ClickHouse. Here we take the local mode as an example.</p><div class="codeBlockContainer_I0IT language-shell theme-code-block"><div class="codeBlockContent_wNvx shell"><pre tabindex="0" class="prism-code language-shell codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/start-seatunnel.sh --config config/batch.conf -e client -m </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;local[2]&#x27;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">‚Äã</a></h2><p>In this post, we covered how to import Nginx log files from HDFS into ClickHouse using Seatunnel. Data can be imported quickly with only one configuration file without writing any code. In addition to supporting HDFS data sources, Seatunnel also supports real-time reading and processing of data from Kafka to ClickHouse. Our next article will describe how to quickly import data from Hive into ClickHouse.</p><p>Of course, Seatunnel is not only a tool for ClickHouse data writing, but also plays a very important role in the writing of data sources such as Elasticsearch and Kafka.</p><p>If you want to know more functions and cases of Seatunnel combined with ClickHouse, Elasticsearch and Kafka, you can go directly to the official website <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><p>-- Power by <a href="https://github.com/InterestingLab" target="_blank" rel="noopener noreferrer">InterestingLab</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/hdfs">HDFS</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/click-house">ClickHouse</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/hive-to-clickhouse">How to quickly import data from Hive into ClickHouse</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-12-30T00:00:00.000Z" itemprop="datePublished">December 30, 2021</time> ¬∑ <!-- -->5 min read</div></header><div class="markdown" itemprop="articleBody"><p>ClickHouse is a distributed columnar DBMS for OLAP. Our department has stored all log data related to data analysis in ClickHouse, an excellent data warehouse, and the current daily data volume has reached 30 billion.</p><p>In the previous article <!-- -->[How to quickly import data from HDFS into ClickHouse]<!-- --> (2021-12-30-hdfs-to-clickhouse.md), we mentioned the use of Seatunnel <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator -seatunnel</a> After a very simple operation on the data in HDFS, the data can be written to ClickHouse. The data in HDFS is generally unstructured data, so what should we do with the structured data stored in Hive?</p><p><img loading="lazy" src="/assets/images/hive-logo-c9aedd90b5ea9668c87fe25ad92a8e6c.png" width="962" height="518"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="hive-to-clickhouse">Hive to ClickHouse<a class="hash-link" href="#hive-to-clickhouse" title="Direct link to heading">‚Äã</a></h2><p>Assuming that our data has been stored in Hive, we need to read the data in the Hive table and filter out the fields we care about, or convert the fields, and finally write the corresponding fields into the ClickHouse table.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="hive-schema">Hive Schema<a class="hash-link" href="#hive-schema" title="Direct link to heading">‚Äã</a></h3><p>The structure of the data table we store in Hive is as follows, which stores common Nginx logs.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE `nginx_msg_detail`(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `hostname` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `domain` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `remote_addr` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `request_time` float,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `datetime` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `url` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `status` int,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `data_size` int,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `referer` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `cookie_info` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `user_agent` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `minute` string)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> PARTITIONED BY (</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `date` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `hour` string)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="clickhouse-schema">ClickHouse Schema<a class="hash-link" href="#clickhouse-schema" title="Direct link to heading">‚Äã</a></h3><p>Our ClickHouse table creation statement is as follows, our table is partitioned by day</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE cms.cms_msg</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date Date,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    datetime DateTime,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    url String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    request_time Float32,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    status String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hostname String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    domain String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remote_addr String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data_size Int32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">) ENGINE = MergeTree PARTITION BY date ORDER BY (date, hostname) SETTINGS index_granularity = 16384</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel-with-clickhouse">Seatunnel with ClickHouse<a class="hash-link" href="#seatunnel-with-clickhouse" title="Direct link to heading">‚Äã</a></h2><p>Next, I will introduce to you how we write data from Hive to ClickHouse through Seatunnel.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel">Seatunnel<a class="hash-link" href="#seatunnel" title="Direct link to heading">‚Äã</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> is a very easy-to-use, high-performance, real-time data processing product that can deal with massive data. It is built on Spark. Seatunnel has a very rich set of plug-ins that support reading data from Kafka, HDFS, and Kudu, performing various data processing, and writing the results to ClickHouse, Elasticsearch or Kafka.</p><p>The environment preparation and installation steps of Seatunnel will not be repeated here. For specific installation steps, please refer to the previous article or visit <a href="/docs/intro/about">Seatunnel Docs</a></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel-pipeline">Seatunnel Pipeline<a class="hash-link" href="#seatunnel-pipeline" title="Direct link to heading">‚Äã</a></h3><p>We only need to write a configuration file of Seatunnel Pipeline to complete the data import.</p><p>The configuration file includes four parts, namely Spark, Input, filter and Output.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="spark">Spark<a class="hash-link" href="#spark" title="Direct link to heading">‚Äã</a></h4><p>This part is the related configuration of Spark, which mainly configures the resource size required for Spark execution.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  // This configuration is required</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.sql.catalogImplementation = &quot;hive&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="input">Input<a class="hash-link" href="#input" title="Direct link to heading">‚Äã</a></h4><p>This part defines the data source. The following is a configuration example of reading data in text format from a Hive file.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hive {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pre_sql = &quot;select * from access.nginx_msg_detail&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>See, a very simple configuration can read data from Hive. <code>pre_sql</code> is the SQL to read data from Hive, and <code>table_name</code> is the name of the table that will register the read data as a temporary table in Spark, which can be any field.</p><p>It should be noted that it must be ensured that the metastore of hive is in the service state.</p><p>When running in Cluster, Client, Local mode, the <code>hive-site.xml</code> file must be placed in the $HADOOP_CONF directory of the submit task node</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="filter">Filter<a class="hash-link" href="#filter" title="Direct link to heading">‚Äã</a></h4><p>In the Filter section, here we configure a series of transformations, and here we discard the unnecessary minute and hour fields. Of course, we can also not read these fields through <code>pre_sql</code> when reading Hive</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remove {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = [&quot;minute&quot;, &quot;hour&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="output">Output<a class="hash-link" href="#output" title="Direct link to heading">‚Äã</a></h4><p>Finally, we write the processed structured data to ClickHouse</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        host = &quot;your.clickhouse.host:8123&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table = &quot;nginx_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields = [&quot;date&quot;, &quot;datetime&quot;, &quot;hostname&quot;, &quot;url&quot;, &quot;http_code&quot;, &quot;request_time&quot;, &quot;data_size&quot;, &quot;domain&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username = &quot;username&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = &quot;password&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="running-seatunnel">Running Seatunnel<a class="hash-link" href="#running-seatunnel" title="Direct link to heading">‚Äã</a></h3><p>We combine the above four-part configuration into our configuration file <code>config/batch.conf</code>.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/batch.conf</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  // This configuration is required</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.sql.catalogImplementation = &quot;hive&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hive {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pre_sql = &quot;select * from access.nginx_msg_detail&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remove {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = [&quot;minute&quot;, &quot;hour&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        host = &quot;your.clickhouse.host:8123&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table = &quot;access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields = [&quot;date&quot;, &quot;datetime&quot;, &quot;hostname&quot;, &quot;uri&quot;, &quot;http_code&quot;, &quot;request_time&quot;, &quot;data_size&quot;, &quot;domain&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username = &quot;username&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = &quot;password&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Execute the command, specify the configuration file, and run Seatunnel to write data to ClickHouse. Here we take the local mode as an example.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/start-seatunnel.sh --config config/batch.conf -e client -m &#x27;local[2]&#x27;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">‚Äã</a></h2><p>In this post, we covered how to import data from Hive into ClickHouse using Seatunnel. The data import can be completed quickly through only one configuration file without writing any code, which is very simple.</p><p>If you want to know more functions and cases of Seatunnel combined with ClickHouse, Elasticsearch, Kafka, Hadoop, you can go directly to the official website <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><p>-- Power by <a href="https://github.com/InterestingLab" target="_blank" rel="noopener noreferrer">InterestingLab</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/hive">Hive</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/click-house">ClickHouse</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/spark-execute-elasticsearch">How to quickly write data to Elasticsearch using Spark</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-12-30T00:00:00.000Z" itemprop="datePublished">December 30, 2021</time> ¬∑ <!-- -->6 min read</div></header><div class="markdown" itemprop="articleBody"><p>When it comes to writing data to Elasticsearch, the first thing that comes to mind must be Logstash. Logstash is accepted by the majority of users because of its simplicity, scalability, and scalability. However, the ruler is shorter and the inch is longer, and Logstash must have application scenarios that it cannot apply to, such as:</p><ul><li>Massive data ETL</li><li>Massive data aggregation</li><li>Multi-source data processing</li></ul><p>In order to meet these scenarios, many students will choose Spark, use Spark operators to process data, and finally write the processing results to Elasticsearch.</p><p>Our department used Spark to analyze Nginx logs, counted our web service access, aggregated Nginx logs every minute and finally wrote the results to Elasticsearch, and then used Kibana to configure real-time monitoring of the Dashboard. Both Elasticsearch and Kibana are convenient and practical, but with more and more similar requirements, how to quickly write data to Elasticsearch through Spark has become a big problem for us.</p><p>Today, I would like to recommend a black technology Seatunnel <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a> that can realize fast data writing. It is very easy to use , a high-performance, real-time data processing product that can deal with massive data. It is built on Spark and is easy to use, flexibly configured, and requires no development.</p><p><img loading="lazy" src="/assets/images/wd-struct-fd963482dc80fdee6e4930107709bd28.png" width="818" height="466"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="kafka-to-elasticsearch">Kafka to Elasticsearch<a class="hash-link" href="#kafka-to-elasticsearch" title="Direct link to heading">‚Äã</a></h2><p>Like Logstash, Seatunnel also supports multiple types of data input. Here we take the most common Kakfa as the input source as an example to explain how to use Seatunnel to quickly write data to Elasticsearch</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="log-sample">Log Sample<a class="hash-link" href="#log-sample" title="Direct link to heading">‚Äã</a></h3><p>The original log format is as follows:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">127.0.0.1 elasticsearch.cn 114.250.140.241 0.001s &quot;127.0.0.1:80&quot; [26/Oct/2018:21:54:32 +0800] &quot;GET /article HTTP/1.1&quot; 200 123 &quot;-&quot; - &quot;Dalvik/2.1.0 (Linux; U; Android 7.1.1; OPPO R11 Build/NMF26X)&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="elasticsearch-document">Elasticsearch Document<a class="hash-link" href="#elasticsearch-document" title="Direct link to heading">‚Äã</a></h3><p>We want to count the visits of each domain name in one minute. The aggregated data has the following fields:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">domain String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hostname String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">status int</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">datetime String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">count int</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel-with-elasticsearch">Seatunnel with Elasticsearch<a class="hash-link" href="#seatunnel-with-elasticsearch" title="Direct link to heading">‚Äã</a></h2><p>Next, I will introduce you in detail, how we read the data in Kafka through Seatunnel, parse and aggregate the data, and finally write the processing results into Elasticsearch.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel">Seatunnel<a class="hash-link" href="#seatunnel" title="Direct link to heading">‚Äã</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> also has a very rich plug-in that supports reading data from Kafka, HDFS, Hive, performing various data processing, and converting the results Write to Elasticsearch, Kudu or Kafka.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="prerequisites">Prerequisites<a class="hash-link" href="#prerequisites" title="Direct link to heading">‚Äã</a></h3><p>First of all, we need to install seatunnel, the installation is very simple, no need to configure system environment variables</p><ol><li>Prepare the Spark environment</li><li>Install Seatunnel</li><li>Configure Seatunnel</li></ol><p>The following are simple steps, the specific installation can refer to <a href="/docs/quick-start">Quick Start</a></p><div class="codeBlockContainer_I0IT language-yaml theme-code-block"><div class="codeBlockContent_wNvx yaml"><pre tabindex="0" class="prism-code language-yaml codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd /usr/local</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">//archive.apache.org/dist/spark/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">bin</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tar </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">xvf https</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">//archive.apache.org/dist/spark/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">bin</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">//github.com/InterestingLab/seatunnel/releases/download/v1.1.1/seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">unzip seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">1.1.1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">env.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Specify the Spark installation path</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">SPARK_HOME=$</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">SPARK_HOME</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">/usr/local/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">bin</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">hadoop2.7</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel-pipeline">Seatunnel Pipeline<a class="hash-link" href="#seatunnel-pipeline" title="Direct link to heading">‚Äã</a></h3><p>Like Logstash, we only need to write a configuration file of Seatunnel Pipeline to complete the data import. I believe that friends who know Logstash can start Seatunnel configuration soon.</p><p>The configuration file includes four parts, namely Spark, Input, filter and Output.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="spark">Spark<a class="hash-link" href="#spark" title="Direct link to heading">‚Äã</a></h4><p>This part is the related configuration of Spark, which mainly configures the resource size required for Spark execution.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.streaming.batchDuration = 5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="input">Input<a class="hash-link" href="#input" title="Direct link to heading">‚Äã</a></h4><p>This part defines the data source. The following is a configuration example of reading data from Kafka,</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    topics = &quot;seatunnel-es&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.group.id = &quot;seatunnel_es_group&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.rebalance.max.retries = 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="filter">Filter<a class="hash-link" href="#filter" title="Direct link to heading">‚Äã</a></h4><p>In the Filter section, here we configure a series of conversions, including regular parsing to split logs, time conversion to convert HTTPDATE to a date format supported by Elasticsearch, type conversion for fields of type Number, and data aggregation through SQL</p><div class="codeBlockContainer_I0IT language-yaml theme-code-block"><div class="codeBlockContent_wNvx yaml"><pre tabindex="0" class="prism-code language-yaml codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Parse the original log using regex</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># The initial data is in the raw_message field</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = &quot;raw_message&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern = &#x27;%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">hostname</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">domain</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">IP</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">remote_addr</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NUMBER</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">request_time</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">s\\s\&quot;%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">upstream_ip</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\&quot;\\s\\</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">HTTPDATE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">timestamp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain">\\s\&quot;%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">method</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">url</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">http_ver</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\&quot;\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NUMBER</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NUMBER</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">body_bytes_send</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">referer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">cookie_info</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s\&quot;%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">user_agent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">&#x27;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Convert data in &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; format to</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># format supported in Elasticsearch</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = &quot;timestamp&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field = &quot;datetime&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format = &quot;dd/MMM/yyyy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">HH</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">mm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">ss Z&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format = &quot;yyyy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">MM</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">dd&#x27;T&#x27;HH</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">mm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">ss.SSS+08</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">00&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)">## Aggregate data with SQL</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = &quot;select domain</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hostname</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> int(status)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> datetime</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> count(</span><span class="token important">*)</span><span class="token plain"> from access_log group by domain</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hostname</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> datetime&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="output">Output<a class="hash-link" href="#output" title="Direct link to heading">‚Äã</a></h4><p>Finally, we write the processed structured data to Elasticsearch.</p><div class="codeBlockContainer_I0IT language-yaml theme-code-block"><div class="codeBlockContent_wNvx yaml"><pre tabindex="0" class="prism-code language-yaml codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    elasticsearch </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        hosts = </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;localhost:9200&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index = &quot;seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">$</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">now</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        es.batch.size.entries = 100000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index_time_format = &quot;yyyy.MM.dd&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="running-seatunnel">Running Seatunnel<a class="hash-link" href="#running-seatunnel" title="Direct link to heading">‚Äã</a></h3><p>We combine the above four-part configuration into our configuration file <code>config/batch.conf</code>.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/batch.conf</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.streaming.batchDuration = 5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topics = &quot;seatunnel-es&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.group.id = &quot;seatunnel_es_group&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.rebalance.max.retries = 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Parse the original log using regex</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # The initial data is in the raw_message field</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = &quot;raw_message&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern = &#x27;%{IP:hostname}\\s%{NOTSPACE:domain}\\s%{IP:remote_addr}\\s%{NUMBER:request_time}s\\s\&quot;%{DATA:upstream_ip}\&quot;\\s\\[%{HTTPDATE:timestamp}\\]\\s\&quot;%{NOTSPACE:method}\\s%{DATA:url}\\s%{NOTSPACE:http_ver}\&quot;\\s%{NUMBER:status}\\s%{NUMBER:body_bytes_send}\\s%{DATA:referer}\\s%{NOTSPACE:cookie_info}\\s\&quot;%{DATA:user_agent}&#x27;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Convert data in &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; format to</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # format supported in Elasticsearch</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = &quot;timestamp&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field = &quot;datetime&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format = &quot;dd/MMM/yyyy:HH:mm:ss Z&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format = &quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:00.SSS+08:00&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ## Aggregate data with SQL</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = &quot;select domain, hostname, status, datetime, count(*) from access_log group by domain, hostname, status, datetime&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    elasticsearch {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        hosts = [&quot;localhost:9200&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index = &quot;seatunnel-${now}&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        es.batch.size.entries = 100000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index_time_format = &quot;yyyy.MM.dd&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Execute the command, specify the configuration file, and run Seatunnel to write data to Elasticsearch. Here we take the local mode as an example.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/start-seatunnel.sh --config config/batch.conf -e client -m &#x27;local[2]&#x27;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Finally, the data written into Elasticsearch is as follows, and with Kibana, real-time monitoring of web services can be realized ^_^.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">&quot;_source&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;domain&quot;: &quot;elasticsearch.cn&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;hostname&quot;: &quot;localhost&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;status&quot;: &quot;200&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;datetime&quot;: &quot;2018-11-26T21:54:00.000+08:00&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;count&quot;: 26</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">‚Äã</a></h2><p>In this post, we introduced how to write data from Kafka to Elasticsearch via Seatunnel. You can quickly run a Spark Application with only one configuration file, complete data processing and writing, and do not need to write any code, which is very simple.</p><p>When there are scenarios that Logstash cannot support or the performance of Logstah cannot meet expectations during data processing, you can try to use Seatunnel to solve the problem.</p><p>If you want to know more functions and cases of using Seatunnel in combination with Elasticsearch, Kafka and Hadoop, you can go directly to the official website <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><p><strong>We will publish another article &quot;How to Use Spark and Elasticsearch for Interactive Data Analysis&quot; in the near future, so stay tuned.</strong></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="contract-us">Contract us<a class="hash-link" href="#contract-us" title="Direct link to heading">‚Äã</a></h2><ul><li>Mailing list : <strong><a href="mailto:dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev@seatunnel.apache.org</a></strong>. Send anything to <code>dev-subscribe@seatunnel.apache.org</code> and subscribe to the mailing list according to the replies.</li><li>Slack: Send a <code>Request to join SeaTunnel slack</code> email to the mailing list (<code>dev@seatunnel.apache.org</code>), and we will invite you to join (please make sure you are registered with Slack before doing so).</li><li><a href="https://space.bilibili.com/1542095008" target="_blank" rel="noopener noreferrer">bilibili B station video</a></li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/spark">Spark</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/kafka">Kafka</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/elasticsearch">Elasticsearch</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/spark-execute-tidb">How to use Spark to do OLAP analysis on TiDB</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-12-30T00:00:00.000Z" itemprop="datePublished">December 30, 2021</time> ¬∑ <!-- -->7 min read</div></header><div class="markdown" itemprop="articleBody"><p><img src="https://download.pingcap.com/images/tidb-planet.jpg"></p><p><a href="https://github.com/pingcap/tidb" target="_blank" rel="noopener noreferrer">TiDB</a> is a fusion database product targeting online transaction processing/online analytical processing. Distributed transactions, real-time OLAP and other important features.</p><p>TiSpark is a product launched by PingCAP to solve the complex OLAP needs of users. It uses the Spark platform and integrates the advantages of TiKV distributed clusters.</p><p>Completing OLAP operations with TiSpark directly requires knowledge of Spark and some development work. So, are there some out-of-the-box tools that can help us use TiSpark to complete OLAP analysis on TiDB more quickly?</p><p>At present, there is a tool <strong>Seatunnel</strong> in the open source community, the project address <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a>, which can be based on Spark, Quickly implement TiDB data reading and OLAP analysis based on TiSpark.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="operating-tidb-with-seatunnel">Operating TiDB with Seatunnel<a class="hash-link" href="#operating-tidb-with-seatunnel" title="Direct link to heading">‚Äã</a></h2><p>We have such a requirement online. Read the website access data of a certain day from TiDB, count the number of visits of each domain name and the status code returned by the service, and finally write the statistical results to another table in TiDB. Let&#x27;s see how Seatunnel implements such a function.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel">Seatunnel<a class="hash-link" href="#seatunnel" title="Direct link to heading">‚Äã</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> is a very easy-to-use, high-performance, real-time data processing product that can deal with massive data. It is built on Spark. Seatunnel has a very rich set of plugins that support reading data from TiDB, Kafka, HDFS, Kudu, perform various data processing, and then write the results to TiDB, ClickHouse, Elasticsearch or Kafka.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="ready-to-work">Ready to work<a class="hash-link" href="#ready-to-work" title="Direct link to heading">‚Äã</a></h4><h5 class="anchor anchorWithStickyNavbar_mojV" id="1-introduction-to-tidb-table-structure">1. Introduction to TiDB table structure<a class="hash-link" href="#1-introduction-to-tidb-table-structure" title="Direct link to heading">‚Äã</a></h5><p><strong>Input</strong> (table where access logs are stored)</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE access_log (</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    domain VARCHAR(255),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    datetime VARCHAR(63),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remote_addr VARCHAR(63),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    http_ver VARCHAR(15),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    body_bytes_send INT,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    status INT,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    request_time FLOAT,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    url TEXT</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">+-----------------+--------------+------+------+---------+-------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| Field           | Type         | Null | Key  | Default | Extra |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+-----------------+--------------+------+------+---------+-------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| domain          | varchar(255) | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| datetime        | varchar(63)  | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| remote_addr     | varchar(63)  | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| http_ver        | varchar(15)  | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| body_bytes_send | int(11)      | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| status          | int(11)      | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| request_time    | float        | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| url             | text         | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+-----------------+--------------+------+------+---------+-------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><strong>Output</strong> (table where result data is stored)</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE access_collect (</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date VARCHAR(23),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    domain VARCHAR(63),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    status INT,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hit INT</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">+--------+-------------+------+------+---------+-------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| Field  | Type        | Null | Key  | Default | Extra |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+--------+-------------+------+------+---------+-------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| date   | varchar(23) | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| domain | varchar(63) | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| status | int(11)     | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| hit    | int(11)     | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+--------+-------------+------+------+---------+-------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="2-install-seatunnel">2. Install Seatunnel<a class="hash-link" href="#2-install-seatunnel" title="Direct link to heading">‚Äã</a></h5><p>After we have the input and output tables of TiDB, we need to install Seatunnel. The installation is very simple, and there is no need to configure system environment variables</p><ol><li>Prepare the Spark environment</li><li>Install Seatunnel</li><li>Configure Seatunnel</li></ol><p>The following are simple steps, the specific installation can refer to <a href="/docs/quick-start">Quick Start</a></p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Download and install Spark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd /usr/local</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tar -xvf https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Download and install seatunnel</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">https://github.com/InterestingLab/seatunnel/releases/download/v1.2.0/seatunnel-1.2.0.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">unzip seatunnel-1.2.0.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd seatunnel-1.2.0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/seatunnel-env.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Specify the Spark installation path</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">SPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.1.0-bin-hadoop2.7}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="implement-the-seatunnel-processing-flow">Implement the Seatunnel processing flow<a class="hash-link" href="#implement-the-seatunnel-processing-flow" title="Direct link to heading">‚Äã</a></h3><p>We only need to write a Seatunnel configuration file to read, process, and write data.</p><p>The Seatunnel configuration file consists of four parts, <code>Spark</code>, <code>Input</code>, <code>Filter</code> and <code>Output</code>. The <code>Input</code> part is used to specify the input source of the data, the <code>Filter</code> part is used to define various data processing and aggregation, and the <code>Output</code> part is responsible for writing the processed data to the specified database or message queue.</p><p>The whole processing flow is <code>Input</code> -&gt; <code>Filter</code> -&gt; <code>Output</code>, which constitutes the processing flow (Pipeline) of Seatunnel.</p><blockquote><p>The following is a specific configuration, which is derived from an online practical application, but simplified for demonstration.</p></blockquote><h5 class="anchor anchorWithStickyNavbar_mojV" id="input-tidb">Input (TiDB)<a class="hash-link" href="#input-tidb" title="Direct link to heading">‚Äã</a></h5><p>This part of the configuration defines the input source. The following is to read data from a table in TiDB.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    tidb {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database = &quot;nginx&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pre_sql = &quot;select * from nginx.access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;spark_nginx_input&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="filter">Filter<a class="hash-link" href="#filter" title="Direct link to heading">‚Äã</a></h5><p>In the Filter section, here we configure a series of transformations, most of the data analysis requirements are completed in the Filter. Seatunnel provides a wealth of plug-ins enough to meet various data analysis needs. Here we complete the data aggregation operation through the SQL plugin.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;spark_nginx_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = &quot;select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)=&#x27;2019-01-20&#x27; group by domain, status, substring(datetime, 1, 10)&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="output-tidb">Output (TiDB)<a class="hash-link" href="#output-tidb" title="Direct link to heading">‚Äã</a></h5><p>Finally, we write the processed results to another table in TiDB. TiDB Output is implemented through JDBC</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    tidb {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        url = &quot;jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&amp;characterEncoding=utf8&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table = &quot;access_collect&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        user = &quot;username&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = &quot;password&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        save_mode = &quot;append&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="spark">Spark<a class="hash-link" href="#spark" title="Direct link to heading">‚Äã</a></h5><p>This part is related to Spark configuration. It mainly configures the resource size required for Spark execution and other Spark configurations.</p><p>Our TiDB Input plugin is implemented based on TiSpark, which relies on TiKV cluster and Placement Driver (PD). So we need to specify PD node information and TiSpark related configuration <code>spark.tispark.pd.addresses</code> and <code>spark.sql.extensions</code>.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel-tidb&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  # Set for TiSpark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.tispark.pd.addresses = &quot;localhost:2379&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.sql.extensions = &quot;org.apache.spark.sql.TiExtensions&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="run-seatunnel">Run Seatunnel<a class="hash-link" href="#run-seatunnel" title="Direct link to heading">‚Äã</a></h4><p>We combine the above four parts into our final configuration file <code>conf/tidb.conf</code></p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.app.name = &quot;seatunnel-tidb&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Set for TiSpark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.tispark.pd.addresses = &quot;localhost:2379&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.sql.extensions = &quot;org.apache.spark.sql.TiExtensions&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    tidb {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database = &quot;nginx&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pre_sql = &quot;select * from nginx.access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;spark_table&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;spark_nginx_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = &quot;select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)=&#x27;2019-01-20&#x27; group by domain, status, substring(datetime, 1, 10)&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    tidb {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        url = &quot;jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&amp;characterEncoding=utf8&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table = &quot;access_collect&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        user = &quot;username&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = &quot;password&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        save_mode = &quot;append&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Execute the command, specify the configuration file, and run Seatunnel to implement our data processing logic.</p><ul><li>Local</li></ul><blockquote><p>./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode client --master &#x27;local<!-- -->[2]<!-- -->&#x27;</p></blockquote><ul><li>yarn-client</li></ul><blockquote><p>./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode client --master yarn</p></blockquote><ul><li>yarn-cluster</li></ul><blockquote><p>./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode cluster -master yarn</p></blockquote><p>If it is a local test and verification logic, you can use the local mode (Local). Generally, in the production environment, the <code>yarn-client</code> or <code>yarn-cluster</code> mode is used.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="test-result">test result<a class="hash-link" href="#test-result" title="Direct link to heading">‚Äã</a></h4><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">mysql&gt; select * from access_collect;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+------------+--------+--------+------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| date       | domain | status | hit  |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+------------+--------+--------+------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| 2019-01-20 | b.com  |    200 |   63 |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| 2019-01-20 | a.com  |    200 |   85 |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+------------+--------+--------+------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2 rows in set (0.21 sec)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">‚Äã</a></h2><p>In this article, we introduced how to use Seatunnel to read data from TiDB, do simple data processing and write it to another table in TiDB. Data can be imported quickly with only one configuration file without writing any code.</p><p>In addition to supporting TiDB data sources, Seatunnel also supports Elasticsearch, Kafka, Kudu, ClickHouse and other data sources.</p><p><strong>At the same time, we are developing an important function, which is to use the transaction features of TiDB in Seatunnel to realize streaming data processing from Kafka to TiDB, and support Exactly-Once data from end (Kafka) to end (TiDB). consistency. </strong></p><p>If you want to know more functions and cases of Seatunnel combined with TiDB, ClickHouse, Elasticsearch and Kafka, you can go directly to the official website <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="contract-us">Contract us<a class="hash-link" href="#contract-us" title="Direct link to heading">‚Äã</a></h2><ul><li>Mailing list : <strong><a href="mailto:dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev@seatunnel.apache.org</a></strong>. Send anything to <code>dev-subscribe@seatunnel.apache.org</code> and subscribe to the mailing list according to the replies.</li><li>Slack: Send a <code>Request to join SeaTunnel slack</code> email to the mailing list (<code>dev@seatunnel.apache.org</code>), and we will invite you to join (please make sure you are registered with Slack before doing so).</li><li><a href="https://space.bilibili.com/1542095008" target="_blank" rel="noopener noreferrer">bilibili B station video</a></li></ul><p>-- Power by <a href="https://github.com/InterestingLab" target="_blank" rel="noopener noreferrer">InterestingLab</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/spark">Spark</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/ti-db">TiDB</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/blog/spark-structured-streaming">How to support Spark StructuredStreaming</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-12-30T00:00:00.000Z" itemprop="datePublished">December 30, 2021</time> ¬∑ <!-- -->8 min read</div></header><div class="markdown" itemprop="articleBody"><h3 class="anchor anchorWithStickyNavbar_mojV" id="foreword">Foreword<a class="hash-link" href="#foreword" title="Direct link to heading">‚Äã</a></h3><p>StructuredStreaming is a newly opened module after Spark 2.0. Compared with SparkStreaming, it has some prominent advantages:<br> <!-- -->‚ÄÉ<!-- -->‚ÄÉ<!-- -->First, it can achieve lower latency;<br>
<!-- -->‚ÄÉ<!-- -->‚ÄÉ<!-- -->Second, real-time aggregation can be done, such as real-time calculation of the total sales of each commodity every day;<br>
<!-- -->‚ÄÉ<!-- -->‚ÄÉ<!-- -->Third, you can do the association between streams, for example, to calculate the click rate of an advertisement, you need to associate the exposure record of the advertisement with the click record. <br>
The above points may be cumbersome or difficult to implement if using SparkStreaming, but it will be easier to implement using StructuredStreaming.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="how-to-use-structuredstreaming">How to use StructuredStreaming<a class="hash-link" href="#how-to-use-structuredstreaming" title="Direct link to heading">‚Äã</a></h3><p>Maybe you have not studied StructuredStreaming in detail, but found that StructuredStreaming can solve your needs very well. How to quickly use StructuredStreaming to solve your needs? Currently there is a tool <strong>Seatunnel</strong> in the community, the project address: <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a> ,
It can help you use StructuredStreaming to complete your needs efficiently and at low cost.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel">Seatunnel<a class="hash-link" href="#seatunnel" title="Direct link to heading">‚Äã</a></h3><p>Seatunnel is a very easy-to-use, high-performance, real-time data processing product that can deal with massive data. It is built on Spark. Seatunnel has a very rich set of plug-ins, supports reading data from Kafka, HDFS, Kudu, performs various data processing, and writes the results to ClickHouse, Elasticsearch or Kafka</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="ready-to-work">Ready to work<a class="hash-link" href="#ready-to-work" title="Direct link to heading">‚Äã</a></h3><p>First we need to install Seatunnel, the installation is very simple, no need to configure system environment variables</p><ol><li>Prepare the Spark environment</li><li>Install Seatunnel</li><li>Configure Seatunnel</li></ol><p>The following are simple steps, the specific installation can refer to <a href="/docs/quick-start">Quick Start</a></p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd /usr/local</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tar -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://github.com/InterestingLab/seatunnel/releases/download/v1.3.0/seatunnel-1.3.0.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">unzip seatunnel-1.3.0.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd seatunnel-1.3.0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/seatunnel-env.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Specify the Spark installation path</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">SPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.2.0-bin-hadoop2.7}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="seatunnel-pipeline">Seatunnel Pipeline<a class="hash-link" href="#seatunnel-pipeline" title="Direct link to heading">‚Äã</a></h3><p>We only need to write a configuration file of Seatunnel Pipeline to complete the data import.</p><p>The configuration file includes four parts, namely Spark, Input, filter and Output.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="spark">Spark<a class="hash-link" href="#spark" title="Direct link to heading">‚Äã</a></h4><p>This part is the related configuration of Spark, which mainly configures the resource size required for Spark execution.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="input">Input<a class="hash-link" href="#input" title="Direct link to heading">‚Äã</a></h4><p>Below is an example of reading data from kafka</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    topics = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    schema = &quot;{\&quot;name\&quot;:\&quot;string\&quot;,\&quot;age\&quot;:\&quot;integer\&quot;,\&quot;addrs\&quot;:{\&quot;country\&quot;:\&quot;string\&quot;,\&quot;city\&quot;:\&quot;string\&quot;}}&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Through the above configuration, the data in kafka can be read. Topics is the topic of kafka to be subscribed to. Subscribing to multiple topics at the same time can be separated by commas. Consumer.bootstrap.servers is the list of Kafka servers, and schema is optional. Because the value read by StructuredStreaming from kafka (official fixed field value) is of binary type, see <a href="http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html" target="_blank" rel="noopener noreferrer">http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html</a>
But if you are sure that the data in your kafka is a json string, you can specify the schema, and the input plugin will parse it according to the schema you specify</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="filter">Filter<a class="hash-link" href="#filter" title="Direct link to heading">‚Äã</a></h4><p>Here is a simple filter example</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;student&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = &quot;select name,age from student&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><code>table_name</code> is the registered temporary table name for easy use in the following sql</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="output">Output<a class="hash-link" href="#output" title="Direct link to heading">‚Äã</a></h4><p>The processed data is output, assuming that our output is also kafka</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafka {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topic = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        producer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        streaming_output_mode = &quot;update&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        checkpointLocation = &quot;/your/path&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><code>topic</code> is the topic you want to output, <code>producer.bootstrap.servers</code> is a list of kafka clusters, <code>streaming_output_mode</code> is an output mode parameter of StructuredStreaming, there are three types of <code>append|update|complete</code>, for details, see the documentation http: //spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes</p><p><code>checkpointLocation</code> is the checkpoint path of StructuredStreaming. If configured, this directory will store the running information of the program. For example, if the program exits and restarts, it will continue to consume the last offset.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="scenario-analysis">Scenario Analysis<a class="hash-link" href="#scenario-analysis" title="Direct link to heading">‚Äã</a></h3><p>The above is a simple example. Next, we will introduce a slightly more complex business scenario.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="scenario-1-real-time-aggregation-scenario">Scenario 1: Real-time aggregation scenario<a class="hash-link" href="#scenario-1-real-time-aggregation-scenario" title="Direct link to heading">‚Äã</a></h4><p>Suppose there is now a mall with 10 kinds of products on it, and now it is necessary to find the daily sales of each product in real time, and even to find the number of buyers of each product (not very precise).
The huge advantage of this is that massive data can be aggregated during real-time processing, and there is no need to write data into the data warehouse first, and then run offline scheduled tasks for aggregation.
It is still very convenient to operate.</p><p>The data of kafka is as follows</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{&quot;good_id&quot;:&quot;abc&quot;,&quot;price&quot;:300,&quot;user_id&quot;:123456,&quot;time&quot;:1553216320}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>So how do we use Seatunnel to fulfill this requirement, of course, we only need to configure it.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">#The configuration in spark is configured according to business requirements</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#configure input</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topics = &quot;good_topic&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        schema = &quot;{\&quot;good_id\&quot;:\&quot;string\&quot;,\&quot;price\&quot;:\&quot;integer\&quot;,\&quot;user_id\&quot;:\&quot;Long\&quot;,\&quot;time\&quot;:\&quot;Long\&quot;}&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#configure filter    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #When the program is doing aggregation, it will internally store the aggregation state of the program since startup, which will lead to OOM over time. If the watermark is set, the program will automatically clean up the state other than the watermark.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #Here means use the ts field to set the watermark, the limit is 1 day</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Watermark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        time_field = &quot;time&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        time_type = &quot;UNIX&quot;              #UNIX represents a timestamp with a time field of 10, and other types can be found in the plugin documentation for details.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        time_pattern = &quot;yyyy-MM-dd&quot;     #The reason why the ts is assigned to the day is because the daily sales are sought, if the hourly sales are sought, the hour can be assigned `yyyy-MM-dd HH`</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        delay_threshold = &quot;1 day&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        watermark_field = &quot;ts&quot;          #After setting the watermark, a new field will be added, `ts` is the name of this field</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #The reason for group by ts is to make the watermark take effect, approx_count_distinct is an estimate, not an exact count_distinct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;good_table_2&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = &quot;select good_id,sum(price) total, approx_count_distinct(user_id) person from good_table_2 group by ts,good_id&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#Next we choose to output the results to Kafka in real time</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafka {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topic = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        producer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        streaming_output_mode = &quot;update&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        checkpointLocation = &quot;/your/path&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>The above configuration is complete, start Seatunnel, and you can get the results you want.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="scenario-2-multiple-stream-association-scenarios">Scenario 2: Multiple stream association scenarios<a class="hash-link" href="#scenario-2-multiple-stream-association-scenarios" title="Direct link to heading">‚Äã</a></h4><p>Suppose you have placed an advertisement on a certain platform, and now you need to calculate the CTR (click-through rate) of each advertisement in real time. The data comes from two topics, one is the advertisement exposure log, and the other is the advertisement click log.
At this point, we need to associate the two stream data together for calculation, and Seatunnel also supports this function recently, let&#x27;s take a look at how to do it:</p><p>Click on topic data format</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{&quot;ad_id&quot;:&quot;abc&quot;,&quot;click_time&quot;:1553216320,&quot;user_id&quot;:12345}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Exposure topic data format</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{&quot;ad_id&quot;:&quot;abc&quot;,&quot;show_time&quot;:1553216220,&quot;user_id&quot;:12345}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#F8F8F2"><span class="token plain">#The configuration in spark is configured according to business requirements</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#configure input</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topics = &quot;click_topic&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        schema = &quot;{\&quot;ad_id\&quot;:\&quot;string\&quot;,\&quot;user_id\&quot;:\&quot;Long\&quot;,\&quot;click_time\&quot;:\&quot;Long\&quot;}&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;click_table&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topics = &quot;show_topic&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        schema = &quot;{\&quot;ad_id\&quot;:\&quot;string\&quot;,\&quot;user_id\&quot;:\&quot;Long\&quot;,\&quot;show_time\&quot;:\&quot;Long\&quot;}&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;show_table&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #Left association right table must set watermark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #Right off left and right tables must set watermark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#inner-joins-with-optional-watermarking</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Watermark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              source_table_name = &quot;click_table&quot; #Here you can specify to add a watermark to a temporary table. If you don&#x27;t specify it, it will be the first one in the input.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              time_field = &quot;time&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              time_type = &quot;UNIX&quot;               </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              delay_threshold = &quot;3 hours&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              watermark_field = &quot;ts&quot; </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              result_table_name = &quot;click_table_watermark&quot; #After adding the watermark, it can be registered as a temporary table, which is convenient for subsequent use in sql</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Watermark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                source_table_name = &quot;show_table&quot; </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                time_field = &quot;time&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                time_type = &quot;UNIX&quot;               </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                delay_threshold = &quot;2 hours&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                watermark_field = &quot;ts&quot; </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                result_table_name = &quot;show_table_watermark&quot; </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;show_table_watermark&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = &quot;select a.ad_id,count(b.user_id)/count(a.user_id) ctr from show_table_watermark as a left join click_table_watermark as b on a.ad_id = b.ad_id and a.user_id = b.user_id &quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#Next we choose to output the results to Kafka in real time</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafka {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topic = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        producer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        streaming_output_mode = &quot;append&quot; #Stream association only supports append mode</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        checkpointLocation = &quot;/your/path&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Through configuration, the case of stream association is also completed here.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">‚Äã</a></h3><p>Through configuration, you can quickly use StructuredStreaming for real-time data processing, but you still need to understand some concepts of StructuredStreaming, such as the watermark mechanism, and the output mode of the program.</p><p>Finally, Seatunnel also supports spark streaming and spark batching of course.
If you are also interested in these two, you can read our previous article &quot;<a href="/blog/hive-to-clickhouse">How to quickly import data from Hive into ClickHouse</a>&quot;,
&quot;<!-- -->[Excellent data engineer, how to use Spark to do OLAP analysis on TiDB]<!-- --> (2021-12-30-spark-execute-tidb.md)&quot;,
&quot;<!-- -->[How to use Spark to quickly write data to Elasticsearch]<!-- --> (2021-12-30-spark-execute-elasticsearch.md)&quot;</p><p>If you want to know more functions and cases of Seatunnel combined with HBase, ClickHouse, Elasticsearch, Kafka, MySQL and other data sources, you can go directly to the official website <!-- -->[https://seatunnel.apache.org/]<!-- -->(<a href="https://seatunnel.apache." target="_blank" rel="noopener noreferrer">https://seatunnel.apache.</a> org/)</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="ËÅîÁ≥ªÊàë‰ª¨">ËÅîÁ≥ªÊàë‰ª¨<a class="hash-link" href="#ËÅîÁ≥ªÊàë‰ª¨" title="Direct link to heading">‚Äã</a></h2><ul><li>Mailing list : <strong><a href="mailto:dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev@seatunnel.apache.org</a></strong>. Send anything to <code>dev-subscribe@seatunnel.apache.org</code> and subscribe to the mailing list according to the replies.</li><li>Slack: Send a <code>Request to join SeaTunnel slack</code> email to the mailing list (<code>dev@seatunnel.apache.org</code>), and we will invite you to join (please make sure you are registered with Slack before doing so).</li><li><a href="https://space.bilibili.com/1542095008" target="_blank" rel="noopener noreferrer">bilibili B station video</a></li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/spark">Spark</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/structured-streaming">StructuredStreaming</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog"><div class="pagination-nav__label">Newer Entries</div></a></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">SeaTunnel</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/faq">FAQ</a></li><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel/releases" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Releases<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Issue Tracker<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel/pulls" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Pull Requests<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">Subscribe Mailing List</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/community/contribution_guide/subscribe">How to Subscribe</a></li><li class="footer__item"><a href="mailto:dev-subscribe@seatunnel.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Subscribe Mail<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://lists.apache.org/list.html?dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Mail Archive<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">
            <div style="margin-top: 20px;background: #f4f8fa">
                <img style="height:50px;margin-bottom: 10px" alt="Apache Software Foundation" src="/image/incubator-logo.svg">
                <p style="color: #999999;font-weight:400;text-align:left">Apache SeaTunnel is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the Apache Incubator. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.</p>
                <div style="border-top: 1px solid #ccc;min-height: 60px;line-height: 20px;text-align: center;font-family: Avenir-Medium;font-size: 14px;color: #999;display: flex;align-items: center;"><span>Copyright ¬© 2021-2023 The Apache Software Foundation. Apache SeaTunnel, SeaTunnel, and its feather logo are trademarks of The Apache Software Foundation.</span></div>
                <div style="text-align: center;">
                    <a href="https://twitter.com/asfseatunnel?s=21" target="_blank" title="Twitter"><svg t="1644553365083" class="icon" viewBox="0 0 1260 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="7015" width="38" height="38"><path d="M1259.846921 121.148242c-46.524504 20.728739-96.273478 34.547899-148.325646 40.536201 53.434084-31.784067 94.430924-82.454319 113.777747-142.797982-50.209613 29.480874-105.486251 51.13089-164.447999 62.646857A257.584528 257.584528 0 0 0 872.449815 0.000276c-142.797982 0-258.418284 115.620302-258.418284 258.418284 0 20.268101 2.303193 40.075563 6.909579 58.961748C405.82286 306.32498 215.579097 203.602561 87.98219 47.446058c-22.110655 38.233008-35.008538 82.454319-35.008538 129.900099 0 89.824537 45.603227 168.593747 115.159663 215.118251-42.378756-1.381916-81.99368-12.897882-117.002217-32.244706v3.224471c0 125.293713 88.90326 229.398049 207.287393 253.351259-21.650017 5.988302-44.681949 9.212773-68.17452 9.212773-16.582991 0-32.705344-1.842555-48.827697-4.606387 32.705344 102.722419 128.518184 177.345881 241.374653 179.649074-88.442621 69.095798-199.917175 110.553277-321.06514 110.553277-20.728739 0-41.457479-1.381916-61.72558-3.685109 114.238386 73.241546 250.126788 116.08094 396.149241 116.08094 475.379089 0 735.179289-393.846048 735.179289-735.179289 0-11.055328-0.460639-22.571294-0.921277-33.626621 51.13089-36.851092 94.891562-82.454319 129.439461-134.045848z" fill="#909094" p-id="7016"></path></svg></a> 
                    <a href="https://join.slack.com/t/apacheseatunnel/shared_invite/zt-123jmewxe-RjB_DW3M3gV~xL91pZ0oVQ" target="_blank" title="Slack" style="margin-left: 20px;"><svg t="1644553076784" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3088" width="36" height="36"><path d="M215.125333 647.04a107.861333 107.861333 0 0 1-107.52 107.648A107.861333 107.861333 0 0 1 0 647.04a107.818667 107.818667 0 0 1 107.605333-107.52h107.52v107.52z m54.229334 0a107.818667 107.818667 0 0 1 107.562666-107.52 107.818667 107.818667 0 0 1 107.562667 107.52v269.354667A107.861333 107.861333 0 0 1 376.917333 1024a107.861333 107.861333 0 0 1-107.562666-107.605333v-269.354667zM376.917333 215.125333a107.861333 107.861333 0 0 1-107.562666-107.52A107.861333 107.861333 0 0 1 376.917333 0a107.861333 107.861333 0 0 1 107.562667 107.605333v107.52H376.917333z m0 54.229334a107.861333 107.861333 0 0 1 107.562667 107.562666 107.861333 107.861333 0 0 1-107.562667 107.562667H107.605333A107.861333 107.861333 0 0 1 0 376.917333a107.861333 107.861333 0 0 1 107.605333-107.562666h269.312z m431.872 107.562666a107.861333 107.861333 0 0 1 107.605334-107.562666A107.861333 107.861333 0 0 1 1024 376.917333a107.861333 107.861333 0 0 1-107.605333 107.562667h-107.605334V376.917333z m-54.101333 0a107.861333 107.861333 0 0 1-107.648 107.562667 107.818667 107.818667 0 0 1-107.52-107.562667V107.605333A107.818667 107.818667 0 0 1 647.04 0a107.861333 107.861333 0 0 1 107.648 107.605333v269.312z m-107.648 431.872a107.861333 107.861333 0 0 1 107.648 107.605334A107.861333 107.861333 0 0 1 647.04 1024a107.818667 107.818667 0 0 1-107.52-107.605333v-107.605334h107.52z m0-54.101333a107.818667 107.818667 0 0 1-107.52-107.648 107.776 107.776 0 0 1 107.52-107.52h269.354667A107.818667 107.818667 0 0 1 1024 647.04a107.861333 107.861333 0 0 1-107.605333 107.648h-269.354667z" p-id="3089" fill="#909094"></path></svg></a> 
                    <a href="https://lists.apache.org/list.html?dev@seatunnel.apache.org" target="_blank" title="Mailing list" style="margin-left: 20px;"><svg t="1644553175467" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5304" width="39" height="39"><path d="M853.333333 170.666667H170.666667c-46.933333 0-85.333333 38.4-85.333334 85.333333v512c0 46.933333 38.4 85.333333 85.333334 85.333333h682.666666c46.933333 0 85.333333-38.4 85.333334-85.333333V256c0-46.933333-38.4-85.333333-85.333334-85.333333z m0 170.666666l-341.333333 213.333334-341.333333-213.333334V256l341.333333 213.333333 341.333333-213.333333v85.333333z" p-id="5305" fill="#909094"></path></svg></a> 
                    <a href="https://github.com/apache/incubator-seatunnel" target="_blank" title="GitHub" style="margin-left: 20px;"><svg t="1644553223000" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6156" width="36" height="36"><path d="M512 12.64c-282.752 0-512 229.216-512 512 0 226.208 146.72 418.144 350.144 485.824 25.6 4.736 35.008-11.104 35.008-24.64 0-12.192-0.48-52.544-0.704-95.328-142.464 30.976-172.512-60.416-172.512-60.416-23.296-59.168-56.832-74.912-56.832-74.912-46.464-31.776 3.52-31.136 3.52-31.136 51.392 3.616 78.464 52.768 78.464 52.768 45.664 78.272 119.776 55.648 148.992 42.56 4.576-33.088 17.856-55.68 32.512-68.48-113.728-12.928-233.28-56.864-233.28-253.024 0-55.904 20-101.568 52.768-137.44-5.312-12.896-22.848-64.96 4.96-135.488 0 0 43.008-13.76 140.832 52.48a491.296 491.296 0 0 1 128.16-17.248c43.488 0.192 87.328 5.888 128.256 17.248 97.728-66.24 140.64-52.48 140.64-52.48 27.872 70.528 10.336 122.592 5.024 135.488 32.832 35.84 52.704 81.536 52.704 137.44 0 196.64-119.776 239.936-233.792 252.64 18.368 15.904 34.72 47.04 34.72 94.816 0 68.512-0.608 123.648-0.608 140.512 0 13.632 9.216 29.6 35.168 24.576C877.472 942.624 1024 750.784 1024 524.64c0-282.784-229.248-512-512-512z" p-id="6157" fill="#909094"></path></svg></a> 
                </div>
            <div></div></div></div></div></div></footer></div>
<script src="/assets/js/runtime~main.cca6282f.js"></script>
<script src="/assets/js/main.80d5c78f.js"></script>
</body>
</html>